{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######  PRAJITA #########\n",
    "### Using RST: RoBERTa ###\n",
    "##########################\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Dataset Path\n",
    "file_path = \"/content/drive/My Drive/Dataset for AI Generated Text Detection/Dataset_for_AI_Generated_Text_Detection.csv\"\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(file_path)\n",
    "print(data.head())  # Verify dataset structure\n",
    "\n",
    "# Basic preprocessing\n",
    "texts = data['text'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "\n",
    "# Split dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to plot ROC curve\n",
    "def plot_roc_curve(y_true, y_prob, model_name):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# RST with H-score (simplified implementation)\n",
    "def compute_h_score(texts1, texts2):\n",
    "    \"\"\"Compute a simplified H-score (e.g., based on word frequency divergence).\"\"\"\n",
    "    def get_word_freq(texts):\n",
    "        word_count = {}\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                word_count[word] = word_count.get(word, 0) + 1\n",
    "        total = sum(word_count.values())\n",
    "        return {k: v / total for k, v in word_count.items()}\n",
    "\n",
    "    freq1 = get_word_freq(texts1)\n",
    "    freq2 = get_word_freq(texts2)\n",
    "\n",
    "    # Hellinger distance as H-score\n",
    "    common_words = set(freq1.keys()).union(freq2.keys())\n",
    "    p = np.array([freq1.get(w, 0) for w in common_words])\n",
    "    q = np.array([freq2.get(w, 0) for w in common_words])\n",
    "    h_score = np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) / np.sqrt(2)\n",
    "    return h_score\n",
    "\n",
    "# Apply RST (rudimentary recursive splitting and H-score comparison)\n",
    "ai_texts = [t for t, l in zip(train_texts, train_labels) if l == 1]\n",
    "non_ai_texts = [t for t, l in zip(train_texts, train_labels) if l == 0]\n",
    "h_score = compute_h_score(ai_texts, non_ai_texts)\n",
    "print(f\"H-score between AI and non-AI texts: {h_score:.4f}\")\n",
    "\n",
    "# RoBERTa Tokenizer and Model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "train_encodings = tokenize_data(train_texts)\n",
    "test_encodings = tokenize_data(test_texts)\n",
    "\n",
    "# Create PyTorch dataset\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TextDataset(train_encodings, train_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "## wandb API key : 270175d47b40dc93d0456ac5b2b6f2df5c07a616 ##\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "pred_probs = torch.softmax(torch.tensor(predictions.predictions), dim=1)[:, 1].numpy()  # Get probabilities for positive class\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, pred_labels, target_names=['Non-AI', 'AI']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(test_labels, pred_labels))\n",
    "\n",
    "# Plot ROC curve\n",
    "plot_roc_curve(test_labels, pred_probs, \"RoBERTa\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
