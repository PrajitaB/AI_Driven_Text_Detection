{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3901a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## Interactive model ##\n",
    "#######################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Dataset Path\n",
    "file_path = \"/content/drive/My Drive/Dataset for AI Generated Text Detection/Dataset_for_AI_Generated_Text_Detection.csv\"\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(file_path)\n",
    "texts = data['text'].values\n",
    "labels = data['label'].values  # 1 = AI-generated, 0 = non-AI-generated\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define H-score function\n",
    "def compute_h_score(texts1, texts2):\n",
    "    \"\"\"Compute a simplified H-score (e.g., based on word frequency divergence).\"\"\"\n",
    "    def get_word_freq(texts):\n",
    "        word_count = {}\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                word_count[word] = word_count.get(word, 0) + 1\n",
    "        total = sum(word_count.values())\n",
    "        return {k: v / total for k, v in word_count.items()}\n",
    "\n",
    "    freq1 = get_word_freq(texts1)\n",
    "    freq2 = get_word_freq(texts2)\n",
    "\n",
    "    # Hellinger distance as H-score\n",
    "    common_words = set(freq1.keys()).union(freq2.keys())\n",
    "    p = np.array([freq1.get(w, 0) for w in common_words])\n",
    "    q = np.array([freq2.get(w, 0) for w in common_words])\n",
    "    h_score = np.sqrt(np.sum((np.sqrt(p) - np.sqrt(q)) ** 2)) / np.sqrt(2)\n",
    "    return h_score\n",
    "\n",
    "# Split training data into AI and non-AI texts\n",
    "ai_texts_train = X_train[y_train == 1]\n",
    "non_ai_texts_train = X_train[y_train == 0]\n",
    "\n",
    "# Train models\n",
    "# TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# SVM\n",
    "svm = SVC(kernel='linear', probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# CNN\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Embedding(5000, 128, input_length=100),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Function to predict user input\n",
    "def predict_text(user_input):\n",
    "    # Prepare input for models\n",
    "    input_tfidf = tfidf.transform([user_input])\n",
    "    input_seq = tokenizer.texts_to_sequences([user_input])\n",
    "    input_pad = pad_sequences(input_seq, maxlen=100)\n",
    "\n",
    "    # H-score prediction\n",
    "    h_score_ai = compute_h_score([user_input], ai_texts_train)\n",
    "    h_score_non_ai = compute_h_score([user_input], non_ai_texts_train)\n",
    "    h_score_pred = 1 if (h_score_ai < h_score_non_ai and h_score_ai > 0.4) else 0\n",
    "    h_score_result = \"AI-generated\" if h_score_pred == 1 else \"Non-AI-generated\"\n",
    "\n",
    "    # SVM prediction\n",
    "    svm_pred = svm.predict(input_tfidf)[0]\n",
    "    svm_result = \"AI-generated\" if svm_pred == 1 else \"Non-AI-generated\"\n",
    "\n",
    "    # Random Forest prediction\n",
    "    rf_pred = rf.predict(input_tfidf)[0]\n",
    "    rf_result = \"AI-generated\" if rf_pred == 1 else \"Non-AI-generated\"\n",
    "\n",
    "    # CNN prediction\n",
    "    cnn_prob = cnn_model.predict(input_pad)[0][0]\n",
    "    cnn_pred = 1 if cnn_prob > 0.5 else 0\n",
    "    cnn_result = \"AI-generated\" if cnn_pred == 1 else \"Non-AI-generated\"\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nPrediction Results for Input Text:\")\n",
    "    print(f\"H-score Method: {h_score_result} (H-score vs AI: {h_score_ai:.4f}, vs Non-AI: {h_score_non_ai:.4f})\")\n",
    "    print(f\"SVM: {svm_result}\")\n",
    "    print(f\"Random Forest: {rf_result}\")\n",
    "    print(f\"CNN: {cnn_result} (Probability: {cnn_prob:.4f})\")\n",
    "\n",
    "# Get user input and predict\n",
    "while True:\n",
    "    user_input = input(\"\\nEnter text to analyze (or 'quit' to exit): \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    predict_text(user_input)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
